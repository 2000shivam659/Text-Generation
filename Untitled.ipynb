{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a711aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ad62c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pragy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests\n",
    "\n",
    "# nltk \n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import string\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc1ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f607ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names=['target', 'ids', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3021124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag   \n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57241d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target           ids\n",
       "count  1.600000e+06  1.600000e+06\n",
       "mean   2.000000e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935761e+08\n",
       "min    0.000000e+00  1.467810e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    2.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0732e8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ade9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values and positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6ac260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0.0\n",
       "ids       0.0\n",
       "date      0.0\n",
       "flag      0.0\n",
       "user      0.0\n",
       "text      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if any null values present\n",
    "(df.isnull().sum() / len(df))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf569f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         1600000\n",
      "No. of positve tagged sentences is:  800000\n",
      "No. of negative tagged sentences is: 800000\n"
     ]
    }
   ],
   "source": [
    "# Count positive vs negative sentences\n",
    "positives = df['target'][df.target == 4]\n",
    "negatives = df['target'][df.target == 0]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(df.shape[0]))\n",
    "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfceb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27b7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas object to a string type\n",
    "df['text'] = df['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2463be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the unnecessary columns\n",
    "df.drop(['ids', 'date', 'flag', 'user'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e99e751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# Define the url pattern\n",
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "some = 'amp,today,tomorrow,going,girl'\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "  tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "  tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "  tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "  tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "  tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "  tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "  tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "  tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "  tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "  tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "  tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "  tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "  tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "  tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "  tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "  tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "  tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "  tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "  tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "  tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "  tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "  tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "  tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "  tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "  tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "  tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "  tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "  tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "  tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "  tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "  tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "  tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "  tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "  tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "  tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "  tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "  tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "  tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "  tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "  tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "  tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "  tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "  tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "  tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "  tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "  tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "  tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "  tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "  tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "  tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "  tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "  tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "  tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "  tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "  tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "  tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "  tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "  tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "  tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "  tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "  tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "  tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "  tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "  tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "  tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "  tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "  tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "  tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "  tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "  tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "  tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "  tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "  tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "  tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "  tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "  tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "  tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "  tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "  tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "  tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "  tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "  tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "  tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "  tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "  tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "  tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "  tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \n",
    "  tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "  tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
    "  tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
    "  tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
    "  tweet = re.sub(r\"2day\", \"today\", tweet)\n",
    "  tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
    "  tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
    "  tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
    "  tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
    "  tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
    "  tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
    "  tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
    "  tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
    "  tweet = re.sub(r\"goood\", \"good\", tweet)\n",
    "  tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "  tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "  tweet = tweet.lower()\n",
    "  tweet=tweet[1:]\n",
    "  # Removing all URls \n",
    "  tweet = re.sub(urlPattern,'',tweet)\n",
    "  # Removing all @username.\n",
    "  tweet = re.sub(userPattern,'', tweet) \n",
    "  #remove some words\n",
    "  tweet= re.sub(some,'',tweet)\n",
    "  #Remove punctuations\n",
    "  tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "  #tokenizing words\n",
    "  tokens = word_tokenize(tweet)\n",
    "  #tokens = [w for w in tokens if len(w)>2]\n",
    "  #Removing Stop Words\n",
    "  final_tokens = [w for w in tokens if w not in stopword]\n",
    "  #reducing a word to its word stem \n",
    "  wordLemm = WordNetLemmatizer()\n",
    "  finalwords=[]\n",
    "  for w in final_tokens:\n",
    "    if len(w)>1:\n",
    "      word = wordLemm.lemmatize(w)\n",
    "      finalwords.append(word)\n",
    "  return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6241d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b116a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev_in_text(tweet):\n",
    "    t=[]\n",
    "    words=tweet.split()\n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
    "    return ' '.join(t)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43bf007a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot awww bummer shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>kenichan dived many time ball managed save 50 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass behaving mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  switchfoot awww bummer shoulda got david carr ...\n",
       "1       0  upset update facebook texting might cry result...\n",
       "2       0  kenichan dived many time ball managed save 50 ...\n",
       "3       0                    whole body feel itchy like fire\n",
       "4       0                   nationwideclass behaving mad see"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the function process_tweets and convert_abbrev_in_text\n",
    "df['text'] = df['text'].apply(process_tweets)\n",
    "df['text'] = df['text'].apply(convert_abbrev_in_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b03286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot awww bummer shoulda david carr third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>kenichan dived many time ball managed save res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass behaving</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0    switchfoot awww bummer shoulda david carr third\n",
       "1       0  upset update facebook texting might result sch...\n",
       "2       0  kenichan dived many time ball managed save res...\n",
       "3       0                    whole body feel itchy like fire\n",
       "4       0                           nationwideclass behaving"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing shortwords\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d84675b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [switchfoot, awww, bummer, shoulda, david, car...\n",
       "1    [upset, update, facebook, texting, might, resu...\n",
       "2    [kenichan, dived, many, time, ball, managed, s...\n",
       "3               [whole, body, feel, itchy, like, fire]\n",
       "4                          [nationwideclass, behaving]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokeniazation\n",
    "tokenized_tweet = df['text'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b92843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c76e28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text']\n",
    "Y = df['target']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84b35418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define a pipeline to vectorize text and train a logistic regression model\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LogisticRegression())])\n",
    "\n",
    "# fit the pipeline on the training data\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6d06706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75    159494\n",
      "           4       0.75      0.78      0.77    160506\n",
      "\n",
      "    accuracy                           0.76    320000\n",
      "   macro avg       0.76      0.76      0.76    320000\n",
      "weighted avg       0.76      0.76      0.76    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# calculate accuracy, F1 score, precision, and recall\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd22d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[117902  41592]\n",
      " [ 34996 125510]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Check how many prediction are correct and incorrect per class\n",
    "print(confusion_matrix(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cc48205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation using GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2118fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# load the tokenizer and the pre-trained GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# set the device to use (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set the model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# generate text from a seed sentence\n",
    "def generate_text(seed_text, max_length=50):\n",
    "    # tokenize the seed text\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # generate text using the GPT-2 model\n",
    "    output = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True)\n",
    "\n",
    "    # decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4db3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08339fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.4.2\n",
      "  Using cached transformers-4.4.2-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (1.24.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (2023.5.5)\n",
      "Requirement already satisfied: requests in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (2.28.2)\n",
      "Collecting sacremoses (from transformers==4.4.2)\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.4.2)\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.4.2) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers==4.4.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.4.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.4.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.4.2) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.4.2) (2022.12.7)\n",
      "Requirement already satisfied: six in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses->transformers==4.4.2) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses->transformers==4.4.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses->transformers==4.4.2) (1.2.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 2.0.0, 2.0.1)\n",
      "ERROR: No matching distribution found for torch==1.4.0\n",
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\pragy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary models\n",
    "\n",
    "!pip3 install transformers==4.4.2\n",
    "!pip3 install torch==1.4.0\n",
    "!pip3 install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b884230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got an internship.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_text:  \n",
      " I got an internship.\n",
      "\n",
      "JARED: It was a big internship, for me.\n",
      "\n",
      "LISLEY: But what I wasn't expecting, or at least was more prepared for, is that you are living in an orphanage\n",
      "BLEU score: 9.039352811507815e-232\n",
      "ROUGE score: 0.11428571183673475\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "import math\n",
    "\n",
    "# take a sample input\n",
    "# generate a sample text\n",
    "seed_text = input()\n",
    "generated_text = generate_text(seed_text)\n",
    "print(\"generated_text: \", '\\n', generated_text)\n",
    "\n",
    "# evaluate BLEU score\n",
    "original_text = 'This is an example sentence.'\n",
    "bleu_score = corpus_bleu([[original_text.split()]], [generated_text.split()])\n",
    "print(f'BLEU score: {bleu_score}')\n",
    "\n",
    "# evaluate ROUGE score\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(generated_text, original_text)\n",
    "rouge_score = scores[0]['rouge-1']['f']\n",
    "print(f'ROUGE score: {rouge_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a03364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d60b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
